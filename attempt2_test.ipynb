{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx2nbe5zZ_-N"
      },
      "source": [
        "# AskReddit Troll Question Detection Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXFHvvFiZ_-U"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fhlIiuWBZ_-U"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDrBqoiNZ_-X",
        "outputId": "c40b4b25-b8cb-49ba-dcd4-cc6a92ab4ced"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk # for tokenizing the paragraphs in sentences and sentences in words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ocHtsZhZ_-X",
        "outputId": "3f4da35a-6e3c-474d-fdc1-e8dc94e362c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/archit/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/archit/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/archit/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rBUkOv3-Z_-Z"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(653061, 2)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# print(test_df.count)\n",
        "# [653061 rows x 3 columns]\n",
        "\n",
        "qid = test_df['qid']\n",
        "\n",
        "test_df.head()\n",
        "test_df.shape\n",
        "# df = test_df[(test_df == 1).any(axis=1)]\n",
        "# print(df['question_text'].tolist())\n",
        "# print(type(qid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J94ZCTaZ_-Z"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLiW_3JiZ_-a"
      },
      "source": [
        "### Dropping the qid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cx1MhOldZ_-b"
      },
      "outputs": [],
      "source": [
        "test_df.drop(columns=[\"qid\"],inplace=True)\n",
        "# test_df.head()\n",
        "\n",
        "sentences = test_df['question_text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ3OL3ubZ_-c",
        "outputId": "6eb2fff1-001b-48f7-ce16-1998c332d3cf"
      },
      "outputs": [],
      "source": [
        "N = 653061\n",
        "sentences = sentences[0:N]\n",
        "# print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-VUUbh5Z_-c"
      },
      "source": [
        "### Cleaning the data\n",
        "\n",
        "- Like removing !?., etc.\n",
        "- converting sentences to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bDlxrDDYZ_-d"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "for sentence in sentences:\n",
        "    temp = re.sub('[^a-zA-Z0-9]', ' ', sentence)\n",
        "    temp = temp.lower()\n",
        "    new_sentence = temp.split()\n",
        "    new_sentence = ' '.join(new_sentence)\n",
        "    sentences[i] = new_sentence\n",
        "    # print(new_sentence)\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dz4BTlbZ_-d"
      },
      "source": [
        "### Lemmatization\n",
        "- We need to perform Stemming and Lemmatization on the sentences. Lemmatization is prefered as of now (Converting to meaningful words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EXYPQfkHZ_-d"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # removing stop words and using list composition \n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "    # joining words using spaces\n",
        "    tokenized_sentences.append(' '.join(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHcmJSTWZ_-e"
      },
      "source": [
        "### Data in sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NVhTdCzmZ_-e"
      },
      "outputs": [],
      "source": [
        "sentences = tokenized_sentences\n",
        "# print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGCFlwCKZ_-e"
      },
      "source": [
        "### Trying Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Q_4MUsZ_-f"
      },
      "source": [
        "#### Bag Of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "H2wvKev9Z_-f"
      },
      "outputs": [],
      "source": [
        "# TODO max_features = 1500 may need to be altered\n",
        "cv = CountVectorizer()\n",
        "X1 = cv.fit_transform(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vmSDQ3SQcgM0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "int64\n",
            "float64\n"
          ]
        }
      ],
      "source": [
        "print(type(X1))\n",
        "print(X1.dtype)\n",
        "X1 = X1.astype(float)\n",
        "print(X1.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DL54RsIZ_-f"
      },
      "source": [
        "#### TF IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mslJKymTZ_-f"
      },
      "outputs": [],
      "source": [
        "cv = TfidfVectorizer()\n",
        "X2 = cv.fit_transform(sentences)\n",
        "# print(X2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Lj34PL32colU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "float64\n"
          ]
        }
      ],
      "source": [
        "print(type(X2))\n",
        "X2 = X2.astype(float)\n",
        "print(X2.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eze130ezqBb_"
      },
      "source": [
        "#### Importing Model and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rn2kPLNFpKIB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6rUXyvQqIm-"
      },
      "source": [
        "#### For data genrated by \"Bag of words\" method  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xuwqd1rAp3dd"
      },
      "outputs": [],
      "source": [
        "lreg1 = joblib.load('Using Split LReg5 Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUYqt4NSqU2n"
      },
      "source": [
        "#### For data generated by \"TD IDF\" method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iGQrUb81p6yD"
      },
      "outputs": [],
      "source": [
        "lreg2 = joblib.load('Using Split LReg6 Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l8Fkxpa16hQ"
      },
      "source": [
        "#### Predict for X1, Y1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LUEcmzS71ogz"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 123404 features, but LogisticRegression is expecting 124033 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31242/2505050943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_yhat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlreg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1668\u001b[0m         )\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0movr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 123404 features, but LogisticRegression is expecting 124033 features as input."
          ]
        }
      ],
      "source": [
        "test_yhat1 = lreg1.predict_proba(X1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE74zYa62MfV"
      },
      "source": [
        "#### Predict for X2, Y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdTbUQBz2QkI"
      },
      "outputs": [],
      "source": [
        "test_yhat2 = lreg2.predict_proba(X2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generating CSV Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_df1 = pd.DataFrame(data=test_yhat1[:,1], columns = [\"target\"])\n",
        "# print(y_pred_df)\n",
        "\n",
        "submission_df1 = pd.concat([qid, y_pred_df1[\"target\"]], axis=1, join='inner')\n",
        "submission_df1.to_csv(\"submission1.csv\", index = False)\n",
        "print(submission_df1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_df2 = pd.DataFrame(data=test_yhat2[:,1], columns = [\"target\"])\n",
        "# print(y_pred_df)\n",
        "\n",
        "submission_df2 = pd.concat([qid, y_pred_df2[\"target\"]], axis=1, join='inner')\n",
        "submission_df2.to_csv(\"submission2.csv\", index = False)\n",
        "print(submission_df2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[\"qid\",\"question_text\"]]\n",
        "for i in range(N):\n",
        "  data.append([qid[i],sentences[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import csv\n",
        "\n",
        "# with open('processed_train_data.csv','w',newline='') as fp:\n",
        "#   a = csv.writer(fp, delimiter=',')\n",
        "#   a.writerows(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import joblib\n",
        "\n",
        "# joblib.dump(lreg1,'Using Split LReg1 Model')\n",
        "# joblib.dump(lreg2,'Using Split LReg2 Model')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "v1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
